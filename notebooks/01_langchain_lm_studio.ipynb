{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanciar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith, a language translation and localization platform, can assist with testing in several ways:\\n\\n1. **Multilingual Testing**: Langsmith allows you to test your application or website in multiple languages simultaneously, ensuring that it functions correctly across different regions and cultures.\\n2. **Translation Validation**: The platform provides human-verified translations, which helps identify potential issues with translation quality, grammar, and syntax, reducing the risk of errors and improving overall user experience.\\n3. **Cultural Sensitivity Testing**: Langsmith's expertise in cultural nuances can help you test your content for cultural sensitivity, ensuring that it is respectful and relevant to different audiences worldwide.\\n4. **Localization Testing**: The platform offers testing services specifically designed for localization, including date, time, and number formatting, currency conversion, and other regional-specific requirements.\\n5. **Automated Testing**: Langsmith's API integration enables automated testing of your application or website, allowing you to test translations, formatting, and functionality in a scalable and efficient manner.\\n\\nBy leveraging Langsmith's services, you can ensure that your product or service is thoroughly tested for linguistic and cultural accuracy, ultimately improving user satisfaction and reducing the risk of errors.\", response_metadata={'token_usage': {'completion_tokens': 235, 'prompt_tokens': 236, 'total_tokens': 471}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3871d5c2-742d-4921-9a1b-b65d83e002b0-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inicio das chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As a world-class technical documentation writer, I'd like to highlight how Langsmith can assist with testing:\\n\\n1. **Automated Testing**: Langsmith's AI-powered language processing capabilities can be integrated with automated testing frameworks to generate test cases and scenarios in multiple languages. This ensures that your software or application is thoroughly tested across different linguistic and cultural contexts.\\n2. **Test Data Generation**: Langsmith can help generate test data, such as user input, error messages, and validation rules, in various languages. This enables you to test your application's language-specific features and edge cases more effectively.\\n3. **Localization Testing**: Langsmith's machine translation capabilities can facilitate localization testing by generating translated versions of your software or application. This allows you to test the localized version with native speakers, ensuring that it meets cultural and linguistic requirements.\\n4. **Error Message Translation**: When errors occur during testing, Langsmith can quickly translate error messages into multiple languages, making it easier to identify and debug issues across different language environments.\\n5. **Test Case Review and Analysis**: Langsmith's natural language processing capabilities can help analyze test cases and review them for consistency, accuracy, and completeness in multiple languages. This ensures that your testing process is thorough and effective across different linguistic contexts.\\n\\nBy leveraging Langsmith's AI-powered language processing capabilities, you can streamline your testing process, reduce errors, and ensure that your software or application meets the needs of a global audience.\", response_metadata={'token_usage': {'completion_tokens': 288, 'prompt_tokens': 289, 'total_tokens': 577}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-288690f4-e541-4f11-a847-0105f7961bce-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a world-class technical documentation writer, I'd like to highlight how Langsmith can assist with testing:\\n\\n1. **Automated Testing**: Langsmith's AI-powered language processing capabilities can be integrated with automated testing frameworks to generate test cases and scenarios in multiple languages. This ensures that your software or application is thoroughly tested across different linguistic and cultural contexts.\\n2. **Test Data Generation**: Langsmith can help generate test data, such as user input, error messages, and validation rules, in various languages. This enables you to test your application's language-specific features and edge cases more effectively.\\n3. **Localization Testing**: Langsmith's machine translation capabilities can facilitate localization testing by providing translated versions of your software or application for testing purposes. This helps identify any issues related to language, culture, or region-specific requirements.\\n4. **Error Message Translation**: When errors occur during testing, Langsmith can quickly translate error messages into multiple languages, making it easier to diagnose and resolve issues in a multilingual environment.\\n5. **Test Case Review and Analysis**: Langsmith's natural language processing capabilities can assist in reviewing and analyzing test cases, identifying potential issues, and providing insights for improvement.\\n\\nBy leveraging Langsmith's features, you can streamline your testing process, reduce the risk of errors, and ensure that your software or application is thoroughly tested across different languages and cultures.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.032019928097724915, 0.06339229643344879, -0.14854365587234497, -0.0749576985836029, 0.05321422964334488, 0.0338364876806736, -0.08441770821809769, 0.040955618023872375, -0.022981032729148865, 0.006492594722658396, -0.002992316149175167, 0.03711792826652527, 0.07530311495065689, 0.03950294107198715, -0.07141084969043732, -0.07568350434303284, 0.03580918163061142, -0.035198185592889786, 0.023276833817362785, 0.05217314511537552, -0.0002876944199670106, -0.03401849791407585, 0.06201925501227379, 0.027593346312642097, 0.03676736727356911, 0.05155615136027336, -0.047059908509254456, 0.04728643223643303, -0.013932494446635246, 0.011554342694580555, 0.019040672108530998, -0.025935795158147812, 0.006764427293092012, -0.04029737412929535, -0.016108892858028412, -0.03797941654920578, 0.07615499198436737, 0.014955293387174606, 0.04871508851647377, 0.057611603289842606, 0.028666773810982704, 0.038383256644010544, -0.041735850274562836, -0.00832691416144371, 0.049295950680971146, 0.023171652108430862, 0.00986748281866312, -0.01431118231266737, -0.016652598977088928, 0.012416494078934193, -0.03893433138728142, -0.04751886799931526, -0.018511317670345306, -0.05232086777687073, 0.046195924282073975, 0.06917084753513336, 0.0937492623925209, -0.025773992761969566, -0.021481497213244438, 0.012911926954984665, 0.067377470433712, 0.010152556002140045, 0.001442924840375781, 0.07585097849369049, 0.017028383910655975, -0.016227880492806435, -0.019826723262667656, 0.03528231382369995, -0.013748735189437866, -0.017204755917191505, 0.06306684762239456, -0.0406479649245739, 0.0384322851896286, -0.00891247671097517, 0.01807788759469986, 0.011891352944076061, 0.035525109618902206, -0.002574007259681821, -0.08805659413337708, 0.03714657947421074, 0.09766079485416412, -0.021410206332802773, 0.061177514493465424, -0.011918880976736546, 0.045707568526268005, 0.007728952914476395, -0.012188633903861046, 0.011665744706988335, -0.032232269644737244, 0.03921094909310341, -0.006842580158263445, 0.03732851892709732, 0.07571962475776672, -0.002514224499464035, -0.027542104944586754, -0.001970091834664345, -0.02159002050757408, -0.01889825612306595, -0.01280091144144535, -0.04478949308395386, -0.03398668020963669, 0.01691267639398575, -0.0005400464870035648, -0.03069419413805008, 0.044743869453668594, 0.08811762928962708, -0.05974743515253067, -0.027550168335437775, -0.06279085576534271, -0.030751999467611313, -0.0319640226662159, -0.02149096317589283, -0.007285606116056442, -0.028616895899176598, -0.008600938133895397, 0.02346637472510338, 0.06846034526824951, -0.017501233145594597, 0.0369880385696888, 0.030875110998749733, -0.027541764080524445, -0.022364700213074684, -0.03035440482199192, 0.05701645836234093, 0.014414878562092781, 0.05646190792322159, -0.037914350628852844, -0.032975926995277405, -0.010003508999943733, 0.017790164798498154, 0.01614561676979065, 0.001184144988656044, -0.0008454550406895578, -0.023480704054236412, -0.017709724605083466, 0.04491732269525528, 0.004686071537435055, -0.044415365904569626, 0.08336939662694931, 0.060510896146297455, 0.09121965616941452, -0.011009400710463524, -0.0038933695759624243, -0.02266653999686241, 0.008374972268939018, -0.007771304342895746, 0.055613067001104355, -0.045257121324539185, -0.061689961701631546, 0.032458528876304626, 0.02651345357298851, 0.03204498812556267, 0.0024922096636146307, 0.06574675440788269, 0.022445829585194588, -0.026603171601891518, -0.013832496479153633, 0.050186194479465485, 0.008830728009343147, 0.01451865117996931, 0.04488459601998329, -0.002504652598872781, -0.05342734232544899, -0.018374605104327202, -0.01872430555522442, -0.020174017176032066, 0.030357548967003822, 0.044515322893857956, 0.018878666684031487, 0.034521523863077164, -0.015498668886721134, -0.03350503742694855, -0.024960866197943687, -0.019367830827832222, 0.036251746118068695, -0.0158841535449028, 0.020800448954105377, -0.03589066490530968, 0.018375376239418983, -0.05090942978858948, 0.03482726216316223, -0.02548373118042946, -0.03880162909626961, 0.038913678377866745, -0.0005442770197987556, -0.009827339090406895, -0.06382530182600021, -0.041743215173482895, -0.010154339484870434, -0.04976776987314224, -0.03725442290306091, 0.029284343123435974, -0.0032960190437734127, 0.010380491614341736, -0.010688582435250282, -0.0392753891646862, 0.05652723088860512, 0.03585103154182434, -0.009976563043892384, -0.04992791637778282, 0.015697678551077843, -0.004950487054884434, -0.049954429268836975, 0.029812170192599297, -0.05648854002356529, 0.00918654166162014, -0.02752583660185337, 0.011217143386602402, 0.0402604341506958, -0.00884409248828888, 0.0682014524936676, 0.010619995184242725, -0.04620322585105896, 0.006481969729065895, -0.03895042464137077, -0.03590823709964752, -0.05066392198204994, -0.03434353321790695, -0.03398040309548378, 0.028878040611743927, 0.03396691754460335, 0.03164484351873398, 0.043087925761938095, -0.044470109045505524, 0.06052568182349205, 0.03956126421689987, -0.005200520157814026, 0.000279147585388273, -0.08121378719806671, 0.020337313413619995, -0.02371371164917946, -0.013066784478724003, 0.06531791388988495, -0.011737008579075336, 0.025206327438354492, 0.0775311216711998, 0.024881111457943916, -0.0009342486155219376, 0.026221876963973045, -0.032909095287323, -0.019704876467585564, 0.02848709002137184, -0.029712088406085968, -0.04738109931349754, -0.028248831629753113, -0.012183220125734806, 0.008042769506573677, -0.03994472324848175, 0.032986029982566833, 0.05304127559065819, 0.012472287751734257, -0.008035029284656048, 0.013988032005727291, -0.019162558019161224, 0.03312929347157478, -0.052122656255960464, 0.02439497970044613, -0.034504372626543045, -0.03495357185602188, -0.05313654989004135, 0.006952907890081406, -0.09911110252141953, 0.02909919247031212, -0.07775140553712845, -0.02283252216875553, -0.03342847153544426, -0.028068501502275467, -0.000287293252767995, 0.03942141681909561, -0.026176685467362404, 0.03305060416460037, 0.049812935292720795, 0.07235819846391678, 0.0026341290213167667, 0.0032619372941553593, -3.4309654438402504e-05, -0.02012679912149906, -0.053781092166900635, -0.00663492176681757, 0.009308959357440472, 0.05157509073615074, -0.05229846388101578, -0.04203576594591141, 0.011731080710887909, -0.019296402111649513, 0.027589501813054085, 0.02987978421151638, 0.0030039718840271235, 0.0015208960976451635, 0.01814115419983864, 0.02767382748425007, 0.015011243522167206, -0.034950874745845795, 0.025086145848035812, 0.01984843611717224, -0.00492439791560173, 0.06820780783891678, 0.002407480962574482, 0.0010353400139138103, -0.002355193719267845, -0.009786246344447136, -0.006206891033798456, 0.03766670823097229, 0.06889587640762329, 0.02918568253517151, 0.019221123307943344, 0.04403345659375191, 0.004893292672932148, 0.07930175960063934, 0.0036608055233955383, -0.024046096950769424, -0.0028349366039037704, -0.006150565575808287, 0.04085914045572281, -0.08600831776857376, 0.0008136943797580898, 0.01293894462287426, 0.007169542834162712, 0.0720241442322731, 0.01205972395837307, 0.0019007964292541146, -0.041814595460891724, -0.008063606917858124, -0.030694186687469482, 0.021487675607204437, -0.006821016781032085, -0.011822300963103771, 0.03927624970674515, 0.007851523347198963, -0.049834735691547394, -0.01779116503894329, 0.04019802808761597, -0.007733156904578209, -0.045995306223630905, -0.044319797307252884, 0.008184990845620632, -0.014123834669589996, 0.026407340541481972, -0.030770234763622284, 0.034145720303058624, 0.07372275739908218, -0.02507716789841652, 0.03199843689799309, -0.05725795403122902, 0.06924474239349365, -0.009879927150905132, -0.03619356453418732, 0.013283656910061836, 0.03749026358127594, -0.0010322313755750656, -0.01111738383769989, -0.05808287858963013, 0.0022974712774157524, 0.05718895420432091, -0.009234585799276829, -0.013903098180890083, -0.00889078713953495, 0.024608779698610306, -0.050489943474531174, 0.004903874825686216, 0.0022257203236222267, -0.03802184760570526, -0.02428452856838703, -0.04412538930773735, 0.018683020025491714, 0.023457420989871025, 0.047064825892448425, 0.02346937544643879, 0.031401269137859344, 0.028391331434249878, -0.004135629627853632, -0.04870416596531868, 0.003276912262663245, 0.011135506443679333, 0.05117914080619812, 0.01528587844222784, -0.04691047593951225, -0.010344263166189194, -0.019977664574980736, 0.01706121489405632, 0.03659050911664963, -0.021386031061410904, -0.017770951613783836, 0.029171058908104897, 0.04230141267180443, 0.009853437542915344, -0.003825691295787692, -0.0049197678454220295, -0.00022093291045166552, -0.01351530198007822, 0.026911955326795578, -0.00595091050490737, -0.04737038165330887, -0.014282923191785812, 0.026895586401224136, 0.0030470136553049088, 0.037830084562301636, -0.05402760952711105, -0.020512936636805534, -0.00020446049165911973, 0.0010385547066107392, -0.03349735587835312, 0.011472191661596298, 0.016620097681879997, -0.02535219117999077, 0.04572153836488724, 0.016486195847392082, -0.06436946988105774, 0.01932639256119728, 0.04607833921909332, -0.03932606428861618, 0.05592195317149162, -0.014920336194336414, -0.05701356753706932, -0.04006670042872429, 0.03311185538768768, 0.04540344700217247, 0.0508946068584919, 0.006382299587130547, 0.01505246665328741, 0.002876756712794304, 0.06275061517953873, -0.01525780837982893, 0.04087144508957863, -0.012697042897343636, -0.002528482349589467, -0.013239575549960136, 0.04643093794584274, -0.019321948289871216, -0.07119988650083542, -0.012127157300710678, -0.01212170161306858, -0.015714215114712715, 0.038827016949653625, 0.027985066175460815, -0.023632504045963287, -0.013986552134156227, -0.017886435613036156, 0.038721680641174316, 0.016211343929171562, 0.04836039990186691, -0.06927680224180222, -0.025300296023488045, 0.012576420791447163, -0.03208034113049507, 0.056937076151371, 0.03183724731206894, -0.004858721047639847, -0.05404360219836235, 0.007080090697854757, -0.08896606415510178, 0.0413094200193882, -0.011274687014520168, 0.021626999601721764, 0.04408927634358406, -0.050063252449035645, -0.01996597833931446, -0.022508960217237473, 0.010487396270036697, 0.02583303302526474, 0.011315721087157726, 0.0012895988766103983, -0.058821920305490494, 0.025410054251551628, 0.033076103776693344, -0.008674311451613903, 0.014930672943592072, -0.013982906006276608, -0.004851017612963915, 0.05060913413763046, 0.014006107114255428, 0.0474543496966362, 0.00857058260589838, 0.0064557380974292755, 0.0034326380118727684, -0.04062756150960922, 0.019001780077815056, -0.028636036440730095, 0.005480926483869553, 0.05512155219912529, -0.010682696476578712, -0.001046577119268477, -0.004122544080018997, -0.05157554894685745, 0.011678633280098438, 0.048070285469293594, 0.011850827373564243, -0.0024019316770136356, 0.002926427870988846, 0.001479458063840866, 0.017878320068120956, 0.005345191340893507, 0.01942475326359272, -0.036322928965091705, 0.01677265390753746, -0.06435146182775497, -0.026613576337695122, 0.009625766426324844, 0.0484684556722641, 0.021926119923591614, -0.03437313064932823, 0.029184499755501747, -0.04222935810685158, 0.03364609554409981, 0.026524916291236877, -0.012980811297893524, -0.060285598039627075, -0.07896240055561066, -0.05076577514410019, 0.008570613339543343, -0.0474105067551136, -0.011677484959363937, 0.004703945014625788, 0.04837494716048241, 0.09767836332321167, -0.03870260715484619, 0.03335029259324074, -0.0050677102990448475, -0.02798599749803543, 0.01665481925010681, 0.010540916584432125, -0.024376988410949707, -0.023121828213334084, -0.037386275827884674, -0.015340132638812065, -0.02661934308707714, 0.013430425897240639, -0.053318459540605545, -0.02589036151766777, -0.021488256752490997, 0.05448879301548004, -0.0034929586108773947, -0.06263480335474014, -0.019055653363466263, -0.02441689744591713, -0.011326465755701065, -0.017857331782579422, 0.004903542809188366, 0.0422372929751873, -0.014661080203950405, -0.036060839891433716, -0.027421819046139717, -0.03308503329753876, 0.0186051856726408, 0.0034490881953388453, -0.005677234847098589, -0.032342106103897095, 0.0538453683257103, 0.016840118914842606, -0.013622787781059742, 0.052390869706869125, -0.03350247070193291, -0.02238372340798378, -0.0030876528471708298, 0.018593046814203262, -0.036372601985931396, -0.046371400356292725, 0.0019213240593671799, -0.021015342324972153, -0.031769394874572754, 0.02572808228433132, 0.002945039886981249, 0.047104962170124054, -0.017751092091202736, -0.06615905463695526, 0.055590782314538956, -0.010253163985908031, 0.02511960081756115, 0.041303083300590515, -0.005680596921592951, 0.011051032692193985, -0.002410055371001363, -0.03656160831451416, -0.05038942024111748, 0.035932574421167374, 0.001982002751901746, 0.008003472350537777, 0.035814717411994934, -0.04314740002155304, -0.08208229392766953, 0.05826874077320099, -0.00869233813136816, -0.03415226563811302, -0.019298721104860306, -0.028031092137098312, -0.04002043604850769, -0.09050112217664719, -0.019072338938713074, 0.03983759135007858, -0.0011100501287728548, 0.0371069610118866, -0.0009824188891798258, -0.02272125519812107, -0.011131585575640202, -0.0028746742755174637, 0.028117308393120766, -0.025981463491916656, -0.063491590321064, 0.014083469286561012, -0.03250601142644882, -0.020927606150507927, -0.0016792512033134699, 0.08761996775865555, -0.017493978142738342, -0.050222571939229965, 0.027182700112462044, 0.05053271725773811, 0.02755691111087799, -0.00760083319619298, -0.0016824743943288922, 0.022548025473952293, 0.026135163381695747, -0.037734463810920715, -0.008268843404948711, 0.03774043172597885, 0.00030771904857829213, 0.06939426064491272, -0.027477920055389404, -0.04845363274216652, 0.02073378674685955, -0.010560905560851097, -0.040309906005859375, -0.00890420749783516, 0.01954268291592598, 0.018982375040650368, -0.007062699180096388, -0.025789376348257065, 0.008560982532799244, -0.0051826369017362595, 0.011347892694175243, -0.01378477830439806, 0.012708613649010658, -0.08953014761209488, -0.01897244155406952, -0.013293039985001087, 0.012998193502426147, -0.03318876400589943, 0.01535781566053629, 0.01180361956357956, 0.019016902893781662, 0.016864681616425514, 0.00405584741383791, 0.003486619796603918, 0.0008552034269087017, 0.01724599488079548, -0.0049410853534936905, 0.08186973631381989, -0.020557643845677376, 0.026167456060647964, -0.04625986889004707, 0.025867026299238205, 0.09388741850852966, -0.01616731658577919, 0.03698661923408508, 0.027296878397464752, -0.027651434764266014, 0.03793416917324066, -0.04419123753905296, -0.06570570915937424, 0.008998386561870575, -0.0018350840546190739, -0.03304135426878929, -0.05470630154013634, 0.03376567363739014, 0.054463330656290054, -0.014994356781244278, 0.0013760864967480302, -0.0653122216463089, -0.04944201558828354, -0.008790457621216774, -0.017308373004198074, 0.012060322798788548, 0.0016176411882042885, -0.03897656127810478, 0.02590409852564335, 0.03779330849647522, 0.007031708024442196, 0.07881058752536774, 0.04339304566383362, 0.028046248480677605, -0.013872357085347176, -0.02394038252532482, -0.015634946525096893, 0.027239082381129265, 0.026584576815366745, -0.10365993529558182, 0.020065944641828537, -0.0579034760594368, -0.0021423124708235264, -0.04518948867917061, 0.011005898006260395, -0.07289176434278488, -0.0008776528993621469, -0.041279811412096024, -0.013257874175906181, -0.043719928711652756, 0.04955263063311577, -0.0002049896284006536, -0.05543461814522743, 0.06079145520925522, -0.0486096628010273, 0.0011513250647112727, 0.04214303940534592, 0.03315189108252525, -0.010148562490940094, -0.013635268434882164, 0.00043023566831834614, 0.03445040434598923, 0.01564348302781582, 0.019639039412140846, -0.0274214968085289, 0.015883643180131912, 0.00032621549325995147, 0.0348624549806118, 0.01740257255733013, -0.05452515184879303, -0.029745547100901604, -0.006828193552792072, -0.03551328554749489, 0.04414043202996254, 0.031204162165522575, 0.008311181329190731, -0.002900033025071025, -0.02416306361556053, 0.009906853549182415, 0.013115530833601952, 0.06039254739880562, -0.04261568561196327, 0.04239949584007263, 0.01646522991359234, 0.02386586368083954, -0.03150642290711403, -0.05940152332186699, 0.01779608428478241, -0.039729151874780655, -0.004085451830178499, -0.03365474194288254, -0.0199427530169487, -0.0428474135696888, -0.02774408459663391, -0.022254260256886482, -0.015281569212675095, 0.0051145246252417564, 0.02543548122048378, 0.019775602966547012, -0.008308268152177334, -0.029042594134807587, 0.010591560043394566, 0.03665175288915634, -0.02366754785180092, 0.008679515682160854, 0.01937553659081459, -0.009524408727884293, -0.028526322916150093, 0.02109519951045513, 0.03604550659656525, -0.00973669346421957, 0.037993304431438446, 0.05770726129412651, -0.031676944345235825, 0.004447632003575563, -0.02534465119242668, 0.05088236182928085, 0.006890459451824427, -0.044224001467227936, -0.02472573146224022, -0.035846151411533356, -0.025175848975777626]\n"
     ]
    }
   ],
   "source": [
    "# Make sure to `pip install openai` first\n",
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "def get_embedding(text, model=\"CompendiumLabs/bge-large-en-v1.5-gguf\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "print(get_embedding(\"Once upon a time, there was a cat.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/tutorials/evaluation\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.embeddings import Embeddings\n",
    "from typing import List\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    \"\"\"Interface for embedding models.\"\"\"\n",
    "    base_url=\"http://localhost:1234/v1\"\n",
    "    api_key=\"lm-studio\"\n",
    "    embed_model=\"CompendiumLabs/bge-large-en-v1.5-gguf\"\n",
    "    client = None\n",
    "\n",
    "    def create_client(self):\n",
    "        self.client = OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed search docs.\"\"\"\n",
    "        if not self.client:\n",
    "            self.create_client()\n",
    "        return [self.client.embeddings.create(input = [text], model=self.embed_model).data[0].embedding for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed query text.\"\"\"\n",
    "        if not self.client:\n",
    "            self.create_client()\n",
    "        return self.client.embeddings.create(input = [text], model=self.embed_model).data[0].embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, CustomEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, Langsmith can help with testing by allowing users to \"visualize test results\".'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the provided context, LangSmith can help with testing by allowing users to:\n",
       "\n",
       "1. Create a dataset: LangSmith allows users to define a dataset with inputs and expected outputs, which can be used to evaluate their application.\n",
       "2. Define metrics: Users can define metrics to evaluate their application's performance, such as correctness and length of the output.\n",
       "3. Run evaluations: LangSmith provides an evaluation framework that allows users to run their application on a dataset and evaluate its performance using defined metrics.\n",
       "4. Compare results: LangSmith enables users to compare the results of different runs by displaying them in the Experiments tab.\n",
       "\n",
       "By using LangSmith, developers can easily test and evaluate their applications, identify areas for improvement, and optimize their models for better performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "display(Markdown(response[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Evaluate your LLM application | ðŸ¦œï¸ðŸ› ï¸ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/tutorials/evaluation', 'title': 'Evaluate your LLM application | ðŸ¦œï¸ðŸ› ï¸ LangSmith', 'description': 'It can be hard to measure the performance of your application with respect to criteria important you or your users.', 'language': 'en'}),\n",
       " Document(page_content=\"Skip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsAdd observability to your LLM applicationEvaluate your LLM applicationOptimize a classifierHow-to guidesConceptsReferencePricingSelf-hostingTutorialsEvaluate your LLM applicationOn this pageEvaluate your LLM applicationIt can be hard to measure the performance of your application with respect to criteria important you or your users.\\nHowever, doing so is crucial, especially as you iterate on your application.\\nIn this guide we will go over how to test and evaluate your application.\\nThis allows you to measure how well your application is performing over a fixed set of data.\\nBeing able to get this insight quickly and reliably will allow you to iterate with confidence.At a high level, in this tutorial we will go over how to:Create an initial golden dataset to measure performanceDefine metrics to use to measure performanceRun evaluations on a few different prompts or modelsCompare results manuallyTrack results over timeSet up automated testing to run in CI/CDFor more information on the evaluation workflows LangSmith supports, check out the how-to guides.Lots to cover, let's dive in!Create a dataset\\u200bThe first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate.\\nThere are a few aspects to consider here:What should the schema of each datapoint be?How many data-points should I gather?How should I gather those data-points?Schema: Each datapoint should consist of, at the very least, the inputs to the application.\\nIf you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output.\\nOften times you cannot define the perfect output - that's okay! Evaluation is an iterative process.\\nSometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent.\\nLangSmith datasets are very flexible and allow you to define arbitrary schemas.How Many: There's no hard and fast rule for how many you should gather.\\nThe main thing to make sure is to make sure you have proper coverage of edge cases you may want to guard against.\\nEven 10-50 examples can provide a lot of value!\\nDon't worry about getting a large number to start - you can (and should) always add over time!How to get: This is maybe the trickiest part.\\nOnce you know you want to gather a dataset... how do you actually go about it?\\nFor most teams that are starting new project, we generally see them start by collecting the first 10-20 data-points by hand.\\nAfter starting with these data-points, these datasets are generally living constructs and grow over time.\\nThey generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set.\\nThere are also methods like synthetically generating data that can be used to augment your dataset.\\nTo start, we recommend not worrying about those and just hand labeling ~10-20 examples.Once you've got your dataset, there are a few different ways to upload them to LangSmith.\\nFor this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).For this tutorial, we will create 5 data-points to evaluate on.\\nWe will be evaluating a question-answering application.\\nThe input will be a question, and the output will be an answer.\\nSince this is a question-answering application, we can define the expected answer.\", metadata={'source': 'https://docs.smith.langchain.com/tutorials/evaluation', 'title': 'Evaluate your LLM application | ðŸ¦œï¸ðŸ› ï¸ LangSmith', 'description': 'It can be hard to measure the performance of your application with respect to criteria important you or your users.', 'language': 'en'}),\n",
       " Document(page_content='The input will be a question, and the output will be an answer.\\nSince this is a question-answering application, we can define the expected answer.\\nLet\\'s show how to create and upload this dataset to LangSmith!from langsmith import Clientclient = Client()# Define dataset: these are your test casesdataset_name = \"QA Example Dataset\"dataset = client.create_dataset(dataset_name)client.create_examples(    inputs=[        {\"question\": \"What is LangChain?\"},        {\"question\": \"What is LangSmith?\"},        {\"question\": \"What is OpenAI?\"},        {\"question\": \"What is Google?\"},        {\"question\": \"What is Mistral?\"},    ],    outputs=[        {\"answer\": \"A framework for building LLM applications\"},        {\"answer\": \"A platform for observing and evaluating LLM applications\"},        {\"answer\": \"A company that creates Large Language Models\"},        {\"answer\": \"A technology company known for search\"},        {\"answer\": \"A company that creates Large Language Models\"},    ],    dataset_id=dataset.id,)Now, if we go the LangSmith UI and look for QA Example Dataset in the Datasets & Testing page,\\nwhen we click into it we should see that we have five new examples.Define metrics\\u200bAfter creating our dataset, we can now define some metrics to evaluate our responses on.\\nSince we have an expected answer, we can compare to that as part of our evaluation.\\nHowever, we do not expect our application to output those exact answers, but rather something that is similar.\\nThis makes our evaluation a little trickier.In addition to evaluating correctness, let\\'s also make sure our answers are short and concise.\\nThis will be a little easier - we can define a simple Python to measure the length of the response.Let\\'s go ahead and define these two metrics.For the first, we will use an LLM to judge whether the output is correct (with respect to the expected output).\\nThis LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function.\\nWe can define our own prompt and LLM to use for evaluation here:from langchain_anthropic import ChatAnthropicfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluator_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students\\' answers to questions.You are grading the following question:{query}Here is the real answer:{answer}You are grading the following predicted answer:{result}Respond with CORRECT or INCORRECT:Grade:\"\"\"PROMPT = PromptTemplate(    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE)eval_llm = ChatAnthropic(temperature=0.0)qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm, \"prompt\": PROMPT})For evaluating the length of the response, this is a lot easier!\\nWe can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.from langsmith.schemas import Run, Exampledef evaluate_length(run: Run, example: Example) -> dict:    prediction = run.outputs.get(\"output\") or \"\"    required = example.outputs.get(\"answer\") or \"\"    score = int(len(prediction) < 2 * len(required))    return {\"key\":\"length\", \"score\": score}Run Evaluations\\u200bGreat! So now how do we run evaluations?\\nNow that we have a dataset and evaluators, all that we need is our application!\\nWe will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM.', metadata={'source': 'https://docs.smith.langchain.com/tutorials/evaluation', 'title': 'Evaluate your LLM application | ðŸ¦œï¸ðŸ› ï¸ LangSmith', 'description': 'It can be hard to measure the performance of your application with respect to criteria important you or your users.', 'language': 'en'}),\n",
       " Document(page_content='We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM.\\nWe will build this using the OpenAI SDK directly:import openaiopenai_client = openai.Client()def my_app(question):    return openai_client.chat.completions.create(        model=\"gpt-3.5-turbo\",        temperature=0,        messages=[            {                \"role\": \"system\",                \"content\": \"Respond to the users question in a short, concise manner (one short sentence).\"            },            {                \"role\": \"user\",                \"content\": question,            }        ],    ).choices[0].message.contentBefore running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call,\\nand then also maps the output of the function to the output key we expect.def langsmith_app(inputs):    output = my_app(inputs[\"question\"])    return {\"output\": output}Great!\\nNow we\\'re ready to run evaluation.\\nLet\\'s do it!from langsmith.evaluation import evaluateexperiment_results = evaluate(    langsmith_app, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    experiment_prefix=\"openai-3.5\", # A prefix for your experiment names to easily identify them)This will output a URL. If we click on it, we should see results of our evaluation!If we go back to the dataset page, we can now see a summary of our one run!Let\\'s now try it out with a different model! Let\\'s try gpt-4-turboimport openaiopenai_client = openai.Client()def my_app_1(question):    return openai_client.chat.completions.create(        model=\"gpt-4-turbo\",        temperature=0,        messages=[            {                \"role\": \"system\",                \"content\": \"Respond to the users question in a short, concise manner (one short sentence).\"            },            {                \"role\": \"user\",                \"content\": question,            }        ],    ).choices[0].message.contentdef langsmith_app_1(inputs):    output = my_app_1(inputs[\"question\"])    return {\"output\": output}from langsmith.evaluation import evaluateexperiment_results = evaluate(    langsmith_app_1, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    experiment_prefix=\"openai-4\", # A prefix for your experiment names to easily identify them)And now let\\'s use GPT-4 but also update the prompt to be a bit more strict in requiring the answer to be short.import openaiopenai_client = openai.Client()def my_app_2(question):    return openai_client.chat.completions.create(        model=\"gpt-4-turbo\",        temperature=0,        messages=[            {                \"role\": \"system\",                \"content\": \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"            },            {                \"role\": \"user\",                \"content\": question,            }        ],    ).choices[0].message.contentdef langsmith_app_2(inputs):    output = my_app_2(inputs[\"question\"])    return {\"output\": output}from langsmith.evaluation import evaluateexperiment_results = evaluate(    langsmith_app_2, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    experiment_prefix=\"strict-openai-4\", # A prefix for your experiment names to easily identify them)If we go back to the experiments tab on the datasets page, we should see that all three runs now show up!Comparing results\\u200bAwesome, we\\'ve evaluated three different runs. But how do we can compare results?\\nThe first way we can do this is just by looking at the runs in the Experiments tab.', metadata={'source': 'https://docs.smith.langchain.com/tutorials/evaluation', 'title': 'Evaluate your LLM application | ðŸ¦œï¸ðŸ› ï¸ LangSmith', 'description': 'It can be hard to measure the performance of your application with respect to criteria important you or your users.', 'language': 'en'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'context': [Document(page_content='Evaluate your LLM application | ðŸ¦œï¸ðŸ› ï¸ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/tutorials/evaluation', 'title': 'Evaluate your LLM application | ðŸ¦œï¸ðŸ› ï¸ LangSmith', 'description': 'It can be hard to measure the performance of your application with respect to criteria important you or your users.', 'language': 'en'}),\n",
       "  Document(page_content=\"Skip to main contentLangSmith API DocsSearchGo to AppQuick startTutorialsAdd observability to your LLM applicationEvaluate your LLM applicationOptimize a classifierHow-to guidesConceptsReferencePricingSelf-hostingTutorialsEvaluate your LLM applicationOn this pageEvaluate your LLM applicationIt can be hard to measure the performance of your application with respect to criteria important you or your users.\\nHowever, doing so is crucial, especially as you iterate on your application.\\nIn this guide we will go over how to test and evaluate your application.\\nThis allows you to measure how well your application is performing over a fixed set of data.\\nBeing able to get this insight quickly and reliably will allow you to iterate with confidence.At a high level, in this tutorial we will go over how to:Create an initial golden dataset to measure performanceDefine metrics to use to measure performanceRun evaluations on a few different prompts or modelsCompare results manuallyTrack results over timeSet up automated testing to run in CI/CDFor more information on the evaluation workflows LangSmith supports, check out the how-to guides.Lots to cover, let's dive in!Create a dataset\\u200bThe first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate.\\nThere are a few aspects to consider here:What should the schema of each datapoint be?How many data-points should I gather?How should I gather those data-points?Schema: Each datapoint should consist of, at the very least, the inputs to the application.\\nIf you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output.\\nOften times you cannot define the perfect output - that's okay! Evaluation is an iterative process.\\nSometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent.\\nLangSmith datasets are very flexible and allow you to define arbitrary schemas.How Many: There's no hard and fast rule for how many you should gather.\\nThe main thing to make sure is to make sure you have proper coverage of edge cases you may want to guard against.\\nEven 10-50 examples can provide a lot of value!\\nDon't worry about getting a large number to start - you can (and should) always add over time!How to get: This is maybe the trickiest part.\\nOnce you know you want to gather a dataset... how do you actually go about it?\\nFor most teams that are starting new project, we generally see them start by collecting the first 10-20 data-points by hand.\\nAfter starting with these data-points, these datasets are generally living constructs and grow over time.\\nThey generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set.\\nThere are also methods like synthetically generating data that can be used to augment your dataset.\\nTo start, we recommend not worrying about those and just hand labeling ~10-20 examples.Once you've got your dataset, there are a few different ways to upload them to LangSmith.\\nFor this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).For this tutorial, we will create 5 data-points to evaluate on.\\nWe will be evaluating a question-answering application.\\nThe input will be a question, and the output will be an answer.\\nSince this is a question-answering application, we can define the expected answer.\", metadata={'source': 'https://docs.smith.langchain.com/tutorials/evaluation', 'title': 'Evaluate your LLM application | ðŸ¦œï¸ðŸ› ï¸ LangSmith', 'description': 'It can be hard to measure the performance of your application with respect to criteria important you or your users.', 'language': 'en'}),\n",
       "  Document(page_content='The input will be a question, and the output will be an answer.\\nSince this is a question-answering application, we can define the expected answer.\\nLet\\'s show how to create and upload this dataset to LangSmith!from langsmith import Clientclient = Client()# Define dataset: these are your test casesdataset_name = \"QA Example Dataset\"dataset = client.create_dataset(dataset_name)client.create_examples(    inputs=[        {\"question\": \"What is LangChain?\"},        {\"question\": \"What is LangSmith?\"},        {\"question\": \"What is OpenAI?\"},        {\"question\": \"What is Google?\"},        {\"question\": \"What is Mistral?\"},    ],    outputs=[        {\"answer\": \"A framework for building LLM applications\"},        {\"answer\": \"A platform for observing and evaluating LLM applications\"},        {\"answer\": \"A company that creates Large Language Models\"},        {\"answer\": \"A technology company known for search\"},        {\"answer\": \"A company that creates Large Language Models\"},    ],    dataset_id=dataset.id,)Now, if we go the LangSmith UI and look for QA Example Dataset in the Datasets & Testing page,\\nwhen we click into it we should see that we have five new examples.Define metrics\\u200bAfter creating our dataset, we can now define some metrics to evaluate our responses on.\\nSince we have an expected answer, we can compare to that as part of our evaluation.\\nHowever, we do not expect our application to output those exact answers, but rather something that is similar.\\nThis makes our evaluation a little trickier.In addition to evaluating correctness, let\\'s also make sure our answers are short and concise.\\nThis will be a little easier - we can define a simple Python to measure the length of the response.Let\\'s go ahead and define these two metrics.For the first, we will use an LLM to judge whether the output is correct (with respect to the expected output).\\nThis LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function.\\nWe can define our own prompt and LLM to use for evaluation here:from langchain_anthropic import ChatAnthropicfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluator_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students\\' answers to questions.You are grading the following question:{query}Here is the real answer:{answer}You are grading the following predicted answer:{result}Respond with CORRECT or INCORRECT:Grade:\"\"\"PROMPT = PromptTemplate(    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE)eval_llm = ChatAnthropic(temperature=0.0)qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm, \"prompt\": PROMPT})For evaluating the length of the response, this is a lot easier!\\nWe can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.from langsmith.schemas import Run, Exampledef evaluate_length(run: Run, example: Example) -> dict:    prediction = run.outputs.get(\"output\") or \"\"    required = example.outputs.get(\"answer\") or \"\"    score = int(len(prediction) < 2 * len(required))    return {\"key\":\"length\", \"score\": score}Run Evaluations\\u200bGreat! So now how do we run evaluations?\\nNow that we have a dataset and evaluators, all that we need is our application!\\nWe will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM.', metadata={'source': 'https://docs.smith.langchain.com/tutorials/evaluation', 'title': 'Evaluate your LLM application | ðŸ¦œï¸ðŸ› ï¸ LangSmith', 'description': 'It can be hard to measure the performance of your application with respect to criteria important you or your users.', 'language': 'en'}),\n",
       "  Document(page_content='We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM.\\nWe will build this using the OpenAI SDK directly:import openaiopenai_client = openai.Client()def my_app(question):    return openai_client.chat.completions.create(        model=\"gpt-3.5-turbo\",        temperature=0,        messages=[            {                \"role\": \"system\",                \"content\": \"Respond to the users question in a short, concise manner (one short sentence).\"            },            {                \"role\": \"user\",                \"content\": question,            }        ],    ).choices[0].message.contentBefore running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call,\\nand then also maps the output of the function to the output key we expect.def langsmith_app(inputs):    output = my_app(inputs[\"question\"])    return {\"output\": output}Great!\\nNow we\\'re ready to run evaluation.\\nLet\\'s do it!from langsmith.evaluation import evaluateexperiment_results = evaluate(    langsmith_app, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    experiment_prefix=\"openai-3.5\", # A prefix for your experiment names to easily identify them)This will output a URL. If we click on it, we should see results of our evaluation!If we go back to the dataset page, we can now see a summary of our one run!Let\\'s now try it out with a different model! Let\\'s try gpt-4-turboimport openaiopenai_client = openai.Client()def my_app_1(question):    return openai_client.chat.completions.create(        model=\"gpt-4-turbo\",        temperature=0,        messages=[            {                \"role\": \"system\",                \"content\": \"Respond to the users question in a short, concise manner (one short sentence).\"            },            {                \"role\": \"user\",                \"content\": question,            }        ],    ).choices[0].message.contentdef langsmith_app_1(inputs):    output = my_app_1(inputs[\"question\"])    return {\"output\": output}from langsmith.evaluation import evaluateexperiment_results = evaluate(    langsmith_app_1, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    experiment_prefix=\"openai-4\", # A prefix for your experiment names to easily identify them)And now let\\'s use GPT-4 but also update the prompt to be a bit more strict in requiring the answer to be short.import openaiopenai_client = openai.Client()def my_app_2(question):    return openai_client.chat.completions.create(        model=\"gpt-4-turbo\",        temperature=0,        messages=[            {                \"role\": \"system\",                \"content\": \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"            },            {                \"role\": \"user\",                \"content\": question,            }        ],    ).choices[0].message.contentdef langsmith_app_2(inputs):    output = my_app_2(inputs[\"question\"])    return {\"output\": output}from langsmith.evaluation import evaluateexperiment_results = evaluate(    langsmith_app_2, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    experiment_prefix=\"strict-openai-4\", # A prefix for your experiment names to easily identify them)If we go back to the experiments tab on the datasets page, we should see that all three runs now show up!Comparing results\\u200bAwesome, we\\'ve evaluated three different runs. But how do we can compare results?\\nThe first way we can do this is just by looking at the runs in the Experiments tab.', metadata={'source': 'https://docs.smith.langchain.com/tutorials/evaluation', 'title': 'Evaluate your LLM application | ðŸ¦œï¸ðŸ› ï¸ LangSmith', 'description': 'It can be hard to measure the performance of your application with respect to criteria important you or your users.', 'language': 'en'})],\n",
       " 'answer': 'LangSmith provides a platform to evaluate and optimize your Large Language Model (LLM) applications. Here\\'s an overview of the process:\\n\\n1. **Create a dataset**: Define a set of test cases, also known as a \"dataset\", that represent the inputs and expected outputs for your LLM application.\\n2. **Define evaluators**: Specify how to evaluate the performance of your LLM application using metrics such as correctness, length, or other custom criteria.\\n3. **Run evaluations**: LangSmith will run your LLM application on the dataset and evaluate its performance using the defined evaluators.\\n4. **Compare results**: View the results of multiple runs with different models, prompts, or configurations to compare their performance.\\n\\nLangSmith supports various features to help you test and optimize your LLM applications:\\n\\n* **Automated testing**: Run evaluations in a continuous integration/continuous deployment (CI/CD) pipeline to ensure consistent quality.\\n* **Custom evaluators**: Define custom evaluation metrics using Python functions or LangSmith\\'s built-in evaluators.\\n* **Multi-model support**: Evaluate multiple models, including different architectures, sizes, and fine-tuning configurations.\\n* **Prompt engineering**: Experiment with different prompts and templates to optimize your LLM application\\'s performance.\\n\\nBy leveraging LangSmith, you can:\\n\\n* Identify areas for improvement in your LLM application\\n* Optimize model performance using various techniques (e.g., prompt engineering, hyperparameter tuning)\\n* Compare the effectiveness of different models or configurations\\n* Ensure consistent quality across multiple runs and environments\\n\\nTo get started with LangSmith, follow these steps:\\n\\n1. Sign up for a LangSmith account.\\n2. Create a new dataset by defining your test cases.\\n3. Define evaluators to measure performance.\\n4. Run evaluations using your LLM application.\\n5. Compare results and optimize your model.\\n\\nIf you have any specific questions or need help with the process, feel free to ask!'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# You need to set OPENAI_API_KEY environment variable or pass it as argument `api_key`.\n",
    "llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangsmith is a powerful tool that can assist with testing in several ways:\n",
      "\n",
      "1. **Automated Testing**: Langsmith can be used to generate test cases and automate the testing process. It can create test data, execute tests, and report results.\n",
      "2. **Code Review**: Langsmith's AI-powered code analysis capabilities can help identify potential issues, such as syntax errors, logical flaws, or security vulnerabilities, in your code before it's even executed.\n",
      "3. **Test Data Generation**: Langsmith can generate test data that is relevant to the specific requirements of your application, making it easier to create comprehensive tests.\n",
      "4. **Test Case Prioritization**: By analyzing the complexity and criticality of different test cases, Langsmith can help you prioritize which tests to run first, ensuring that the most important ones are executed.\n",
      "5. **Regression Testing**: Langsmith's ability to analyze code changes and identify potential regressions can help you detect issues that may have been introduced during updates or refactoring.\n",
      "6. **Test Automation Framework Integration**: Langsmith can integrate with popular test automation frameworks like Selenium, Appium, or JUnit, allowing you to leverage its capabilities within your existing testing infrastructure.\n",
      "\n",
      "By leveraging these features, Langsmith can significantly streamline and improve the efficiency of your testing process, helping you deliver higher-quality software faster and more reliably.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can langsmith help with testing?',\n",
       " 'output': \"Langsmith is a powerful tool that can assist with testing in several ways:\\n\\n1. **Automated Testing**: Langsmith can be used to generate test cases and automate the testing process. It can create test data, execute tests, and report results.\\n2. **Code Review**: Langsmith's AI-powered code analysis capabilities can help identify potential issues, such as syntax errors, logical flaws, or security vulnerabilities, in your code before it's even executed.\\n3. **Test Data Generation**: Langsmith can generate test data that is relevant to the specific requirements of your application, making it easier to create comprehensive tests.\\n4. **Test Case Prioritization**: By analyzing the complexity and criticality of different test cases, Langsmith can help you prioritize which tests to run first, ensuring that the most important ones are executed.\\n5. **Regression Testing**: Langsmith's ability to analyze code changes and identify potential regressions can help you detect issues that may have been introduced during updates or refactoring.\\n6. **Test Automation Framework Integration**: Langsmith can integrate with popular test automation frameworks like Selenium, Appium, or JUnit, allowing you to leverage its capabilities within your existing testing infrastructure.\\n\\nBy leveraging these features, Langsmith can significantly streamline and improve the efficiency of your testing process, helping you deliver higher-quality software faster and more reliably.\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAccording to current weather reports, San Francisco (SFO) has:\n",
      "\n",
      "**Current Weather:** Partly Cloudy\n",
      "\n",
      "**Temperature:** 58Â°F (14Â°C)\n",
      "\n",
      "**Humidity:** 64%\n",
      "\n",
      "**Wind:** Light breeze at 7 mph (11 km/h)\n",
      "\n",
      "**UV Index:** Moderate (5 out of 10)\n",
      "\n",
      "Please note that weather conditions can change rapidly, so it's always a good idea to check the latest forecast before heading out. Would you like me to provide more information or help with anything else?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the weather in SF?',\n",
       " 'output': \"According to current weather reports, San Francisco (SFO) has:\\n\\n**Current Weather:** Partly Cloudy\\n\\n**Temperature:** 58Â°F (14Â°C)\\n\\n**Humidity:** 64%\\n\\n**Wind:** Light breeze at 7 mph (11 km/h)\\n\\n**UV Index:** Moderate (5 out of 10)\\n\\nPlease note that weather conditions can change rapidly, so it's always a good idea to check the latest forecast before heading out. Would you like me to provide more information or help with anything else?\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is the weather in SF?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RatelimitException",
     "evalue": "https://links.duckduckgo.com/d.js?q=what+is+the+weather+in+SF%3F&kl=wt-wt&l=wt-wt&p=&s=0&df=y&vqd=4-304725744095816561580234963354421099979&ex=-1 202 Ratelimit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRatelimitException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is the weather in SF?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/site-packages/langchain_core/tools.py:452\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(observation, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/site-packages/langchain_core/tools.py:409\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m     parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(tool_input)\n\u001b[1;32m    407\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    408\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 409\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run, \u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[1;32m    414\u001b[0m     )\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error:\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/site-packages/langchain_community/tools/ddg_search/tool.py:39\u001b[0m, in \u001b[0;36mDuckDuckGoSearchRun._run\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     35\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     36\u001b[0m     run_manager: Optional[CallbackManagerForToolRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/site-packages/langchain_community/utilities/duckduckgo_search.py:81\u001b[0m, in \u001b[0;36mDuckDuckGoSearchAPIWrapper.run\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run query through DuckDuckGo and return concatenated results.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 81\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ddgs_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     83\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ddgs_news(query)\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/site-packages/langchain_community/utilities/duckduckgo_search.py:48\u001b[0m, in \u001b[0;36mDuckDuckGoSearchAPIWrapper._ddgs_text\u001b[0;34m(self, query, max_results)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mduckduckgo_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDGS\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DDGS() \u001b[38;5;28;01mas\u001b[39;00m ddgs:\n\u001b[0;32m---> 48\u001b[0m     ddgs_gen \u001b[38;5;241m=\u001b[39m \u001b[43mddgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafesearch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafesearch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimelimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ddgs_gen:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ddgs_gen]\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search.py:57\u001b[0m, in \u001b[0;36mDDGS.text\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_async_in_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search.py:53\u001b[0m, in \u001b[0;36mDDGS._run_async_in_thread\u001b[0;34m(self, coro)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs an async coroutine in a separate thread.\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m future: Future[Any] \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop)\n\u001b[0;32m---> 53\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search_async.py:167\u001b[0m, in \u001b[0;36mAsyncDDGS.text\u001b[0;34m(self, keywords, region, safesearch, timelimit, backend, max_results)\u001b[0m\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml is not installed. Using backend=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_api(keywords, region, safesearch, timelimit, max_results)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    169\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_html(keywords, region, safesearch, timelimit, max_results)\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search_async.py:258\u001b[0m, in \u001b[0;36mAsyncDDGS._text_api\u001b[0;34m(self, keywords, region, safesearch, timelimit, max_results)\u001b[0m\n\u001b[1;32m    256\u001b[0m         task\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks, return_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(islice(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, results), max_results))\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search_async.py:253\u001b[0m, in \u001b[0;36mAsyncDDGS._text_api\u001b[0;34m(self, keywords, region, safesearch, timelimit, max_results)\u001b[0m\n\u001b[1;32m    249\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m    250\u001b[0m         asyncio\u001b[38;5;241m.\u001b[39mcreate_task(_text_api_page(s, i)) \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m23\u001b[39m, max_results, \u001b[38;5;241m50\u001b[39m), start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    251\u001b[0m     )\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks:\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search_async.py:229\u001b[0m, in \u001b[0;36mAsyncDDGS._text_api.<locals>._text_api_page\u001b[0;34m(s, page)\u001b[0m\n\u001b[1;32m    227\u001b[0m priority \u001b[38;5;241m=\u001b[39m page \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    228\u001b[0m payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 229\u001b[0m resp_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aget_url(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://links.duckduckgo.com/d.js\u001b[39m\u001b[38;5;124m\"\u001b[39m, params\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[1;32m    230\u001b[0m page_data \u001b[38;5;241m=\u001b[39m _text_extract_json(resp_content, keywords)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m page_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/lc-study/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search_async.py:124\u001b[0m, in \u001b[0;36mAsyncDDGS._aget_url\u001b[0;34m(self, method, url, data, params)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_event\u001b[38;5;241m.\u001b[39mset()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m202\u001b[39m, \u001b[38;5;241m301\u001b[39m, \u001b[38;5;241m403\u001b[39m):\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RatelimitException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Ratelimit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m DuckDuckGoSearchException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m return None. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRatelimitException\u001b[0m: https://links.duckduckgo.com/d.js?q=what+is+the+weather+in+SF%3F&kl=wt-wt&l=wt-wt&p=&s=0&df=y&vqd=4-304725744095816561580234963354421099979&ex=-1 202 Ratelimit"
     ]
    }
   ],
   "source": [
    "search.run(\"what is the weather in SF?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
